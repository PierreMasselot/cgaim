#!#############################################################################
#!
#!                       Secondary functions (internally used)                 
#!
#!#############################################################################

#! distributed lags
dlag <- function(x, lags = 1, na.action = na.pass)
# x: vector of data to lag
# lags: vector of lags to apply
# na.action: action for the NAs generated by the lagging
{
    n <- length(x)
    L <- length(lags)
    maxlag <- max(lags)
    res <- matrix(NA, n, L)
    for (j in 1:L){
        lj <- lags[j]
        res[(lj+1):n,j] <- x[1:(n-lj)]
    }
    res <- do.call(na.action, list(res))
    return(res)
}

L2 <- function(y, yhat, w){ 
    return(weighted.mean((y-yhat)^2, w))
}

#! Initialize alpha coefficients in a single-index model
alpha.init <- function(p, y, x, type = c("constant", "runif", "rnorm", "ols"), normalize = TRUE, first.pos = TRUE)
# type: character giving the type of initialization, "contant" attributes the same value to each alpha, "rnorm" and "runif" generates alphas by drawing random numbers respectively from a standard normal distribution and a uniform distribution, "ols" uses ols estimates of y ~ x as initial values
# p: number of coefficients, used only for type = "constant" and "random"
# y, x: response and predictors used for type = "ols"
# normalize: if TRUE the resulting alphas are normalized such that the vector norm is one
# first.pos: if TRUE, the first weight is forced to be positive

# Value: a vector of alpha coefficients of length p (or ncol(x) if type = "ols")
{
    type <- match.arg(type)
    if (type %in% c("constant", "rnorm", "runif") && missing(p)) stop("p is missing")
    if (type == "ols" && (missing(y) || missing(x))) stop("x and y must be provided when type = 'ols'")
    alpha <- switch(type,
        constant = rep(1, p),
        rnorm = rnorm(p),
        runif = runif(p)-.5,
        ols = coef(lm(y ~ 0 + x))
    )
    if (normalize) alpha <- alpha/sqrt(sum(alpha^2))
    if (first.pos) alpha <- sign(alpha[1])*alpha
    return(alpha)
}

#! Derivation by first differences
ppr.der <- function(x, z, gz, alpha){ #! TODO add pooling as in Friedman (1984) or litterature review for numeric derivation
    zord <- order(z)
    gzs <- gz[zord]
    xs <- x[zord,]
    dgzs <- (gzs[3:n] - gzs[1:(n-2)]) / ((xs[3:n,] - xs[1:(n-2),])%*%alpha)
    dgzs <- c(dgzs[1], dgzs, dgzs[n-2])
    dgz <- dgzs[rank(z)]
}

#! Different smoothers
#! Retirer alpha?
spline.smoother <- function(x, y, alpha, w, ...){
    z <- x %*% alpha
    sm.fit <- smooth.spline(z, y, w = w, ...)
    gz <- predict(sm.fit, z)$y
    dgz <- predict(sm.fit, z, deriv = 1)$y
    return(list(gz = gz, dgz = dgz))
}

super.smoother <- function(x, y, alpha, w, ...){
    z <- x %*% alpha
    gz <- supsmu(z, y, wt = w, ...)$y[rank(z)]
    dgz <- ppr.der(x = x, z = z, gz = gz, alpha = alpha)
    return(list(gz = gz, dgz = dgz))
}

constrained.smoother <- function(x, y, alpha, w, const, ...){
    z <- x %*% alpha
}

smooth.conspline <- function(x, y = NULL, lambda = 1, ...){
    B <- ns(x, df = 10)
    nb <- ncol(B)
    Sigma <- matrix(1, nb, nb)
    Sigma[upper.tri(Sigma)] <- 0
    X <- B %*% Sigma
    penMat <- matrix(0, nb-2, nb)
    penMat[row(penMat) == (col(penMat) - 1)] <- 1
    penMat[row(penMat) == (col(penMat) - 2)] <- -1
    
}