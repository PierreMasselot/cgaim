#!#############################################################################
#!
#!                       Secondary functions (internally used)                 
#!
#!#############################################################################

#! distributed lags
dlag <- function(x, lags = 1, na.action = na.pass)  #! ADD TIME INDEX
# x: vector of data to lag
# lags: vector of lags to apply
# na.action: action for the NAs generated by the lagging
{
    n <- length(x)
    L <- length(lags)
    maxlag <- max(lags)
    res <- matrix(NA, n, L)
    for (j in 1:L){
        lj <- lags[j]
        res[(lj+1):n,j] <- x[1:(n-lj)]
    }
    res <- do.call(na.action, list(res))
    return(res)
}

dlag_len <- function(x, lag, na.action = na.pass)
# x: vector of data to lag
# lag: maximum lag
# na.action: action for the NAs generated by the lagging
{
  lags <- 0:lag
  return(dlag(x, lags, na.action))
}


#! L2 loss function
L2 <- function(y, yhat, w){ 
    n <- length(y)
    if (missing(w)) w <- rep(1 / n, n)
    return(weighted.mean((y-yhat)^2, w))
}




#! Derivation by first differences
ppr.der <- function(x, z, gz, alpha){ #! TODO add pooling as in Friedman (1984) or litterature review for numeric derivation
    zord <- order(z)
    gzs <- gz[zord]
    xs <- x[zord,]
    dgzs <- (gzs[3:n] - gzs[1:(n-2)]) / ((xs[3:n,] - xs[1:(n-2),])%*%alpha)
    dgzs <- c(dgzs[1], dgzs, dgzs[n-2])
    dgz <- dgzs[rank(z)]
}

#! Different smoothers
#! Retirer alpha?
spline.smoother <- function(z, y, w, ...){
    sm.fit <- smooth.spline(z, y, w = w, ...)
    gz <- predict(sm.fit, z)$y
    dgz <- predict(sm.fit, z, deriv = 1)$y
    return(list(gz = gz, dgz = dgz))
}

super.smoother <- function(z, y, w, ...){
    gz <- supsmu(z, y, wt = w, ...)$y[rank(z)]
    dgz <- ppr.der(x = x, z = z, gz = gz, alpha = get("alpha", envir = parent.frame()))
    return(list(gz = gz, dgz = dgz))
}

scam.smoother <- function(z, y, w, const = "mpi", df = -1, ord = NA, lambda = NULL, ...){
    fit <- scam(y ~ s(z, bs = const, k = df, m = ord, sp = lambda), data = data.frame(y, z), weights = w, ...)
    gz <- predict(fit, data.frame(z = z), type = "response")
    dgz <- derivative.scam(fit)
    return(list(gz = as.vector(gz), dgz = as.vector(dgz$d)))
}

###############################################################################
#
#                           Alpha
#
###############################################################################

#' Initialization of alphas
#'
#' Initialize alpha coefficients for GAIM fitting.
#'
#' @param y numeric vector containing the response
#' @param x numeric matrix containing the variables of the index
#' @param w numeric vector of weights
#' @param type Character giving the type of initialization, "contant" attributes the same value to each alpha, "random" generates alphas by drawing random numbers from a uniform distribution, "regression" uses lm
#' @param norm.type character giving the norm used for normalizing alphas
#' @sign.const indicates if there is a sign constraint on alphas
#' @monotone initialized alphas are monotonically increasing or decreasing
alpha.init <- function(y, x, w, type = c("constant", "random", "regression"),
  norm.type = "L2", sign.const = 0, monotone = 0)

{
    type <- match.arg(type)
    x <- as.matrix(x)
    p <- ncol(x)
    alpha <- switch(type,
      constant = rep(1, p),
      random = runif(p),
      regression = coef(lm(y ~ 0 + x)) 
    )
    if (sign.const != 0) alpha <- alpha * sign(alpha) * sign.const
    if (monotone != 0) alpha <- sort(alpha, decreasing = monotone == -1)
    alpha <- normalize(alpha, norm.type)
    return(alpha)
}

#' Gauss-Newton update for alphas.
#'
#' Computes the update of alphas in the GAIM by in a Gauss-Newton algorithm.
#'
#' @param r Numeric vector. The residual of the current Gauss-Newton step.
#' @param x List of the index matrices. 
#' @param dgz Numeric matrix containing the derivative of current smooth
#'    functions.
#' @param alpha List of the current values of alpha coefficients.
#' @param w Numeric vector of weights.
#' @param delta Logical. If TRUE, the change is computed instead of the
#'    new alphas directly.
#' @param monotone Integer vector indicating if alphas are constrained to be
#'    monotone. 0 indicates no monotonicity, -1 decreasing and 1 increasing.
#'    Recycled if of a different length than dg.
#' @param sign.const Integer vector indicating a constraint on the sign of
#'    alphas. 0 indicates no constraint, -1 that all alphas must be negative 
#'    and 1 that all alphas must be positive. Recycled if of a different length 
#'    than dg.
#' @param constraint.algo Character indicating which method is used to contraint 
#'    monotonicity of parameters. "QP" indicates classical quadratic programming
#'    and "pya" indicates the method used in the SCAM (Pya and Wood, 2015).
#! POTENTIALLY RIDGE REGRESSION OR LASSO? SEE ROOSEN AND HASTIE (1994) AND SEARCH LITTERATURE
gn_update <- function(r, x, dgz, alpha = rep(0, sum(pvec)), 
  w = rep(1 / length(y), length(y)), delta = TRUE, monotone = 0, 
  sign.const = 0, constraint.algo = c("QP", "reparam"), norm.type = "L2",
  solver = NULL, qp_pars = list())
{
  constraint.algo <- match.arg(constraint.algo)
  if(is.matrix(x)) x <- list(x)
  p <- length(x)
  pvec <- sapply(x, ncol)
  pind <- rep(1:p, pvec)
  alphavec <- unlist(alpha)
  # prepare predictors
  V <- Map("*", x, as.data.frame(dgz))
  Vmat <- Reduce("cbind", V)
  # Prepare constraints
  Amat <- const.matrix(pind, monotone, sign.const)
  # prepare response
  if (!delta){ 
    r <- r + Vmat %*% alphavec
    l <- rep(0, nrow(Amat)) 
  } else {
    l <- -(Amat %*% alphavec)
  }
  if (any(monotone != 0 | sign.const != 0)){   
    if (any(!monotone %in% -1:1 | !sign.const %in% -1:1)){
      stop("monotone and sign.const must be one of c(-1, 0, 1)")
    }
    alpha.up <- switch(constraint.algo,
      QP = update.QP(r, V, w, Amat, l, solver, qp_pars),
      reparam = update.reparam(r, V, w, monotone, sign.const)
    )
  } else {
    alpha.up <- coef(lm(r ~ 0 + Vmat, weights = w))
  }
  if (!delta) alpha.up <- alphavec - alpha.up
  alpha.up <- split(alpha.up, pind)
#  alpha.new <- Map(normalize, alpha.up, norm.type)
  return(alpha.up)
}

#' @param x List of numeric matrices.
update.QP <- function(y, x, w, Amat, low, solver = c("osqp", "quadprog"), 
  qp_pars = list())
{
  solver <- match.arg(solver)
  p <- length(x)
  pvec <- sapply(x, ncol)
  pind <- rep(1:p, pvec)
#  pind1 <- rep(1:p, pvec - 1)
  xall <- Reduce("cbind", x)
  W <- diag(w)
  Dmat <- t(xall) %*% W %*% xall
  dvec <- 2 * t(y) %*% W %*% xall
  # OSQP
  def_settings <- list(verbose = FALSE)
  qp_pars <- c(qp_pars, 
    def_settings[!names(def_settings) %in% names(qp_pars)])
  low <- low + max(c(formals(osqpSettings)$eps_abs, qp_pars$eps_abs))
  sol <- switch(solver,
    osqp = osqp::solve_osqp(P = Dmat, q = -dvec, A = Amat, 
      l = low, u = rep(Inf, nrow(Amat)), pars = qp_pars)$x,
    quadprog = quadprog::solve.QP(Dmat, dvec, t(Amat), bvec = low)$solution
  )
  return(sol)
}

const.matrix <- function(pind, monotone, sign.const, first.pos = T){
  pind1 <- pind[c(diff(pind) == 0, FALSE)]
  Sigma <- matrix(0, length(pind) - 1, length(pind))
  diag(Sigma) <- 1
  Sigma[col(Sigma) - row(Sigma)  == 1] <- -1
  Sigma <- Sigma[diff(pind) == 0,, drop = FALSE]
  Sigma[monotone[pind1] == 1,] <- -1 * Sigma[monotone[pind1] == 1,]
  Sigma <- Sigma[monotone[pind1] != 0,, drop = FALSE]
  csign <- sign.const[pind]
  if (first.pos){
    pfirst <- which(diff(c(0, pind)) != 0)
    pfirst <- pfirst[sign.const > -1]
    csign[pfirst] <- 1
  }
  Asign <- diag(csign)
  Asign <- Asign[apply(Asign, 1, sum) != 0,, drop = F]
  return(rbind(Sigma, Asign))
}

#! TODO: General reparametrization
update.reparam <- function(y, x, w, monotone, sign.const){
  p <- length(x)
  pvec <- sapply(x, ncol)
  pind <- rep(1:p, pvec)
  SigList <- mapply(function(p, m){
    sig <- diag(p)
    if (m == -1){
      sig[lower.tri(sig, diag = TRUE)] <- -1
      sig[,1] <- 1    
    }
    if (m == 1){
      sig[lower.tri(sig)] <- 1
    }
    return(sig)
  }, pvec, monotone)
  Sigma <- Matrix::bdiag(SigList)
  xall <- Reduce("cbind", x)
  xall <- as.matrix(xall %*% Sigma)
  delta <- coef(lm(y ~ 0 + xall, weights = w))
  inds <- which(monotone[pind] != 0)
  inds <- inds[!inds %in% c(1, which(diff(pind) != 0) + 1)]
  delta[inds] <- exp(delta[inds])
  delta <- Sigma %*% delta
  return(delta)
}

#' Normalization of a vector of alphas
#'
#' Provides different ways to normalize a vector.
#'
#' @param alpha Numeric vector to normalize.
#' @param type Character indicating the type of normalization. List...
normalize <- function(alpha, type = "L2"){
  anorm <- switch(type,
    L2 = norm(alpha, "2"),
    L1 = norm(as.matrix(alpha), "1"),
    sum = sum(alpha),
    1
  )
  out <- if (anorm != 0) alpha / anorm else alpha
  attr(out, "norm") <- anorm
  return(out)
}


###############################################################################
#
#                                Smoothing
#
###############################################################################

#' Estimate ridge functions g
#'
#' Performs scatterplot smoothing to obtain ridge functions g.
#'
#' @param x A matrix or data frame giving indices (one per column).
#' @param y A numeric vector containing the output of the model.
#' @param w A numeric vector containing weights.
#' @param method A character value giving the method for smoothing.
#' @param shape A character vector indicating the type of smoothing for each
#'    index. Recycled if not the same length as \code{ncol(x)}. Can be one of
#'    the smoothers available in the \code{mgcv} package (see 
#'    code{\link[mgcv]{smooth.terms}}). Can also be one of the 
#'    shaped-constrained smoothers in \code{scam} (see 
#'    code{\link[scam]{shape.constrained.smooth.termss}}).
#' @param ... Additional arguments to be passed to the method.
smoothing <- function(x, y, w, method = "gam", shape = "tp", ...)
{
  dots <- list(...)
  x <- as.data.frame(x)
  n <- length(y)
  p <- ncol(x)
  if (method == "gam" && 
    any(shape %in% c("mpi", "mpd", "cx", "cv", "micx", "micv", "mdcx", "mdcv")))
    method <- "scam"
  shape <- rep_len(shape, p)
  #--- SCAM
  if (method == "scam"){
    form.rhs <- sprintf("s(%s, bs = '%s')", colnames(x), shape)
    form <- sprintf("y ~ %s", paste(form.rhs, collapse = " + "))
    scam.pars <- c(list(formula = as.formula(form), 
      data = data.frame(y = y, x), weights = w), dots)
    gfit <- do.call(scam::scam, scam.pars)
    gx <- predict(gfit, type = "terms")
    dmod <- lapply(1:p, derivative.scam, object = gfit)
    dgx <- sapply(dmod, "[[", "d")
    beta0 <- coef(gfit)[1]
  } else { if (method == "gam"){
    form.rhs <- sprintf("s(%s, bs = '%s')", colnames(x), shape)
    form <- sprintf("y ~ %s", paste(form.rhs, collapse = " + "))
    gam.pars <- c(list(formula = as.formula(form), 
      data = data.frame(y = y, x), weights = w), dots)
    gfit <- do.call(mgcv::gam, gam.pars)
    gx <- predict(gfit, type = "terms")
    dmod <- gratia::fderiv(gfit, newdata = x)  #! github package gratia. May want to change this line if the package is not available on CRAN when the paper is published
    dgx <- sapply(dmod$derivatives, "[[", "deriv")
    beta0 <- coef(gfit)[1]
  } else { if(method == "scar"){
    scar.pars <- c(list(x = data.matrix(x), y = y, shape = shape, weights = w), 
      dots)
    gfit <- do.call(scar::scar, scar.pars)
    gx <- gfit$componentfit
    dgx <- mapply(function(x, gx) splinefun(x, gx)(x, deriv = 1), 
      as.data.frame(x), as.data.frame(gx))
    beta0 <- gfit$constant
  }}}
  return(list(intercept = beta0, gz = gx, dgz = dgx))  
}
