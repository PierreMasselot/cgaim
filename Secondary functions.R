#!#############################################################################
#!
#!                       Secondary functions (internally used)                 
#!
#!#############################################################################

#! distributed lags
dlag <- function(x, lags = 1, na.action = na.pass)
# x: vector of data to lag
# lags: vector of lags to apply
# na.action: action for the NAs generated by the lagging
{
    n <- length(x)
    L <- length(lags)
    maxlag <- max(lags)
    res <- matrix(NA, n, L)
    for (j in 1:L){
        lj <- lags[j]
        res[(lj+1):n,j] <- x[1:(n-lj)]
    }
    res <- do.call(na.action, list(res))
    return(res)
}

L2 <- function(y, yhat, w){ 
    return(weighted.mean((y-yhat)^2, w))
}

#! Initialize alpha coefficients in a single-index model
alpha.init <- function(p, y, x, type = c("constant", "runif", "rnorm", "ols"), normalize = TRUE, first.pos = TRUE)
# type: character giving the type of initialization, "contant" attributes the same value to each alpha, "rnorm" and "runif" generates alphas by drawing random numbers respectively from a standard normal distribution and a uniform distribution, "ols" uses ols estimates of y ~ x as initial values
# p: number of coefficients, used only for type = "constant" and "random"
# y, x: response and predictors used for type = "ols"
# normalize: if TRUE the resulting alphas are normalized such that the vector norm is one
# first.pos: if TRUE, the first weight is forced to be positive

# Value: a vector of alpha coefficients of length p (or ncol(x) if type = "ols")
{
    type <- match.arg(type)
    if (type %in% c("constant", "rnorm", "runif") && missing(p)) stop("p is missing")
    if (type == "ols" && (missing(y) || missing(x))) stop("x and y must be provided when type = 'ols'")
    alpha <- switch(type,
        constant = rep(1, p),
        rnorm = rnorm(p),
        runif = runif(p)-.5,
        ols = coef(lm(y ~ 0 + x))
    )
    if (normalize) alpha <- alpha/sqrt(sum(alpha^2))
    if (first.pos) alpha <- sign(alpha[1])*alpha
    return(alpha)
}

#! Derivation by first differences
ppr.der <- function(x, z, gz, alpha){ #! TODO add pooling as in Friedman (1984) or litterature review for numeric derivation
    zord <- order(z)
    gzs <- gz[zord]
    xs <- x[zord,]
    dgzs <- (gzs[3:n] - gzs[1:(n-2)]) / ((xs[3:n,] - xs[1:(n-2),])%*%alpha)
    dgzs <- c(dgzs[1], dgzs, dgzs[n-2])
    dgz <- dgzs[rank(z)]
}

#! Different smoothers
#! Retirer alpha?
spline.smoother <- function(z, y, w, ...){
    sm.fit <- smooth.spline(z, y, w = w, ...)
    gz <- predict(sm.fit, z)$y
    dgz <- predict(sm.fit, z, deriv = 1)$y
    return(list(gz = gz, dgz = dgz))
}

super.smoother <- function(z, y, w, ...){
    gz <- supsmu(z, y, wt = w, ...)$y[rank(z)]
    dgz <- ppr.der(x = x, z = z, gz = gz, alpha = get("alpha", envir = parent.frame()))
    return(list(gz = gz, dgz = dgz))
}

scam.smoother <- function(z, y, w, const = "mpi", df = -1, ord = NA, lambda = NULL, ...){
    fit <- scam(y ~ s(z, bs = const, k = df, m = ord, sp = lambda), data = data.frame(y, z), weights = w, ...)
    gz <- predict(fit, data.frame(z = z), type = "response")
    dgz <- derivative.scam(fit)
    return(list(gz = as.vector(gz), dgz = as.vector(dgz$d)))
}

